{
	"jobConfig": {
		"name": "nyc-taxi-mdm-dimensions",
		"description": "",
		"role": "arn:aws:iam::474668386387:role/service-role/AWSGlueServiceRole-nandni",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "nyc-taxi-mdm-dimensions.py",
		"scriptLocation": "s3://aws-glue-assets-474668386387-us-west-2/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2026-01-15T02:30:14.527Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-474668386387-us-west-2/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-474668386387-us-west-2/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "# =====================================================\n# Glue Job: nyc-taxi-mdm-dimensions\n# Purpose : Master Data Management (SCD Type 2)\n# Rerun-safe version with dynamic SCD and vendor names\n# =====================================================\n\nfrom awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import IntegerType, DateType\nimport boto3\n\n# -----------------------------------------------------\n# Setup\n# -----------------------------------------------------\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\n\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n\n# -----------------------------------------------------\n# Paths\n# -----------------------------------------------------\nCURATED_PATH = \"s3://my-data-lake-lab-nandnioubt/curated/nyc_taxi/\"\nZONE_LOOKUP_PATH = \"s3://my-data-lake-lab-nandnioubt/reference/nyc_taxi/taxi_zone_lookup.csv\"\nMASTER_BASE_PATH = \"s3://my-data-lake-lab-nandnioubt/master/redshift/\"\n\n# -----------------------------------------------------\n# Helpers\n# -----------------------------------------------------\ndef path_exists(path):\n    s3 = boto3.client(\"s3\")\n    bucket, prefix = path.replace(\"s3://\", \"\").split(\"/\", 1)\n    resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=1)\n    return \"Contents\" in resp\n\ndef safe_read_parquet(path, schema=None):\n    \"\"\"\n    Safely read parquet from S3. Returns empty DF if missing or folder is empty.\n    \"\"\"\n    s3 = boto3.client(\"s3\")\n    bucket, prefix = path.replace(\"s3://\", \"\").split(\"/\", 1)\n    resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n    if \"Contents\" not in resp or len(resp[\"Contents\"]) == 0:\n        return spark.createDataFrame([], schema) if schema else None\n    files = [obj[\"Key\"] for obj in resp[\"Contents\"] if obj[\"Key\"].endswith(\".parquet\")]\n    if not files:\n        return spark.createDataFrame([], schema) if schema else None\n    return spark.read.parquet(path)\n\ndef align_columns(df, columns):\n    \"\"\"Ensure df has exactly the specified columns; add missing as nulls\"\"\"\n    for col in columns:\n        if col not in df.columns:\n            df = df.withColumn(col, F.lit(None))\n    return df.select(columns)\n\n# -----------------------------------------------------\n# Read curated data\n# -----------------------------------------------------\ncurated_df = safe_read_parquet(CURATED_PATH)\nif curated_df is None or curated_df.rdd.isEmpty():\n    raise Exception(f\"No curated data found at {CURATED_PATH}\")\nprint(\"Curated dataset count:\", curated_df.count())\n\n# -----------------------------------------------------\n# DIM_VENDOR with vendor_name\n# -----------------------------------------------------\n# Vendor ID â†’ Name mapping\nvendor_lookup = spark.createDataFrame([\n    (1, \"Creative Mobile Technologies, LLC\"),\n    (2, \"Curb Mobility, LLC\"),\n    (6, \"Myle Technologies Inc\"),\n    (7, \"Helix\")\n], [\"vendor_id\", \"vendor_name\"])\n\n# Extract distinct vendor IDs\ndim_vendor_new = (\n    curated_df\n    .select(F.col(\"vendorid\").alias(\"vendor_id\"))\n    .distinct()\n    .filter(F.col(\"vendor_id\").isNotNull())\n)\n\n# Join with lookup to get vendor names\ndim_vendor_new = dim_vendor_new.join(vendor_lookup, on=\"vendor_id\", how=\"left\")\n\n# Fill unknown vendors if any new IDs appear\ndim_vendor_new = dim_vendor_new.withColumn(\n    \"vendor_name\",\n    F.coalesce(F.col(\"vendor_name\"), F.lit(\"Unknown\"))\n)\n\nprint(\"dim_vendor_new count:\", dim_vendor_new.count())\n\n# -----------------------------------------------------\n# DIM_RATE_CODE\n# -----------------------------------------------------\ndim_rate_new = (\n    curated_df\n    .select(F.col(\"ratecodeid\").alias(\"rate_code_id\"))\n    .distinct()\n    .filter(F.col(\"rate_code_id\").isNotNull())\n)\nprint(\"dim_rate_new count:\", dim_rate_new.count())\n\n# -----------------------------------------------------\n# DIM_ZONE (union pickup + dropoff)\n# -----------------------------------------------------\nlocation_ids = (\n    curated_df.select(F.col(\"pulocationid\").alias(\"location_id\"))\n    .union(curated_df.select(F.col(\"dolocationid\").alias(\"location_id\")))\n    .distinct()\n)\nprint(\"Unique location_ids count:\", location_ids.count())\n\nzone_ref_df = (\n    spark.read.option(\"header\", True)\n    .csv(ZONE_LOOKUP_PATH)\n    .withColumn(\"zone_location_id\", F.col(\"LocationID\").cast(IntegerType()))\n    .withColumnRenamed(\"Borough\", \"zone_borough\")\n    .withColumnRenamed(\"Zone\", \"zone_name\")\n    .withColumnRenamed(\"service_zone\", \"zone_service_zone\")\n    .drop(\"LocationID\")\n)\n\ndim_zone_new = (\n    location_ids.join(\n        zone_ref_df,\n        location_ids.location_id == zone_ref_df.zone_location_id,\n        \"left\"\n    )\n    .drop(\"zone_location_id\")\n)\nprint(\"dim_zone_new count:\", dim_zone_new.count())\n\n# -----------------------------------------------------\n# Generic dynamic SCD Type-2\n# -----------------------------------------------------\ndef scd_type2(new_df, master_path, business_key, surrogate_key):\n    \"\"\"\n    Dynamic SCD Type-2 loader that keeps all business columns\n    and safely handles incremental changes with column alignment.\n    \"\"\"\n    today = F.current_date()\n    window = Window.orderBy(business_key)\n\n    # Read old master if exists\n    old_df = safe_read_parquet(master_path)\n    if old_df is None or old_df.rdd.isEmpty():\n        old_df = None\n\n    # First run: no master exists\n    if old_df is None:\n        final_df = (\n            new_df\n            .withColumn(surrogate_key, F.row_number().over(window))\n            .withColumn(\"effective_start_date\", today)\n            .withColumn(\"effective_end_date\", F.lit(\"9999-12-31\").cast(DateType()))\n            .withColumn(\"is_current\", F.lit(True))\n            .withColumn(\"created_ts\", F.current_timestamp())\n            .withColumn(\"updated_ts\", F.current_timestamp())\n        )\n        return final_df\n\n    # Incremental run: master exists\n    current_df = old_df.filter(F.col(\"is_current\") == True)\n\n    # Identify new or changed business keys\n    changes_df = new_df.join(current_df.select(business_key), business_key, \"left_anti\")\n    max_sk = old_df.agg(F.max(surrogate_key)).collect()[0][0] or 0\n\n    # New records\n    new_records = (\n        changes_df\n        .withColumn(surrogate_key, F.row_number().over(window) + max_sk)\n        .withColumn(\"effective_start_date\", today)\n        .withColumn(\"effective_end_date\", F.lit(\"9999-12-31\").cast(DateType()))\n        .withColumn(\"is_current\", F.lit(True))\n        .withColumn(\"created_ts\", F.current_timestamp())\n        .withColumn(\"updated_ts\", F.current_timestamp())\n    )\n\n    # Expire changed records\n    expired_records = (\n        current_df.join(changes_df.select(business_key), business_key, \"inner\")\n        .withColumn(\"effective_end_date\", today)\n        .withColumn(\"is_current\", F.lit(False))\n        .withColumn(\"updated_ts\", F.current_timestamp())\n    )\n\n    # Keep unchanged records\n    unchanged = old_df.join(changes_df.select(business_key), business_key, \"left_anti\")\n\n    # Align all columns before union\n    all_cols = list(set(unchanged.columns) | set(new_records.columns) | set(expired_records.columns))\n    unchanged = align_columns(unchanged, all_cols)\n    expired_records = align_columns(expired_records, all_cols)\n    new_records = align_columns(new_records, all_cols)\n\n    # Combine all\n    final_df = unchanged.unionByName(expired_records).unionByName(new_records)\n\n    return final_df\n\n# -----------------------------------------------------\n# Apply SCD Type-2 to all dimensions\n# -----------------------------------------------------\ndim_vendor_final = scd_type2(\n    dim_vendor_new,\n    MASTER_BASE_PATH + \"dim_vendor/\",\n    \"vendor_id\",\n    \"vendor_sk\"\n)\ndim_rate_final = scd_type2(\n    dim_rate_new,\n    MASTER_BASE_PATH + \"dim_rate_code/\",\n    \"rate_code_id\",\n    \"rate_code_sk\"\n)\ndim_zone_final = scd_type2(\n    dim_zone_new,\n    MASTER_BASE_PATH + \"dim_zone/\",\n    \"location_id\",\n    \"zone_sk\"\n)\n\n# -----------------------------------------------------\n# Print counts\n# -----------------------------------------------------\nprint(\"MDM dimension load completed successfully.\")\nprint(\"dim_vendor_final count:\", dim_vendor_final.count())\nprint(\"dim_rate_final count:\", dim_rate_final.count())\nprint(\"dim_zone_final count:\", dim_zone_final.count())\n\n# -----------------------------------------------------\n# Write master tables safely\n# -----------------------------------------------------\nprint(\"Writing master dimensions to S3...\")\ndim_vendor_final.write.mode(\"overwrite\").parquet(MASTER_BASE_PATH + \"dim_vendor/\")\ndim_rate_final.write.mode(\"overwrite\").parquet(MASTER_BASE_PATH + \"dim_rate_code/\")\ndim_zone_final.write.mode(\"overwrite\").parquet(MASTER_BASE_PATH + \"dim_zone/\")\nprint(\"Master dimension tables written successfully.\")\n"
}