{
	"jobConfig": {
		"name": "nyc-taxi-trip-fact",
		"description": "",
		"role": "arn:aws:iam::474668386387:role/service-role/AWSGlueServiceRole-nandni",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "nyc-taxi-trip-fact.py",
		"scriptLocation": "s3://aws-glue-assets-474668386387-us-west-2/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2026-01-20T01:28:20.875Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-474668386387-us-west-2/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-474668386387-us-west-2/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "from awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import DataFrame\n\n# -----------------------------\n# Glue / Spark setup\n# -----------------------------\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\n\n# -----------------------------\n# Config\n# -----------------------------\nS3_RAW = \"s3://my-data-lake-lab-nandnioubt/curated/nyc_taxi/\"\nS3_FACT_STAGING = \"s3://my-data-lake-lab-nandnioubt/master/redshift/trip_fact\"\nDIM_VENDOR = \"s3://my-data-lake-lab-nandnioubt/master/redshift/dim_vendor\"\nDIM_ZONE = \"s3://my-data-lake-lab-nandnioubt/master/redshift/dim_zone\"\nDIM_RATE_CODE = \"s3://my-data-lake-lab-nandnioubt/master/redshift/dim_rate_code\"\n\n# -----------------------------\n# Read source trip data\n# -----------------------------\ndf_trip = spark.read.parquet(S3_RAW)\n\n# -----------------------------\n# Read dimension tables\n# -----------------------------\ndf_vendor = spark.read.parquet(DIM_VENDOR).select(\"vendor_id\", \"vendor_sk\")\ndf_zone = spark.read.parquet(DIM_ZONE).select(\"location_id\", \"zone_sk\")\ndf_rate = spark.read.parquet(DIM_RATE_CODE).select(\"rate_code_id\", \"rate_code_sk\")\n\n# -----------------------------\n# Join to get surrogate keys\n# -----------------------------\ndf_fact = (\n    df_trip\n    .join(df_vendor, df_trip.vendorid == df_vendor.vendor_id, \"left\")\n    .join(df_rate, df_trip.ratecodeid == df_rate.rate_code_id, \"left\")\n    .join(df_zone.withColumnRenamed(\"location_id\", \"PULocationID\").withColumnRenamed(\"zone_sk\", \"pickup_zone_sk\"),\n          on=\"PULocationID\", how=\"left\")\n    .join(df_zone.withColumnRenamed(\"location_id\", \"DOLocationID\").withColumnRenamed(\"zone_sk\", \"dropoff_zone_sk\"),\n          on=\"DOLocationID\", how=\"left\")\n)\n\n# -----------------------------\n# Select and rename columns for fact table\n# -----------------------------\ndf_fact_final = df_fact.select(\n    \"vendor_sk\",\n    \"pickup_zone_sk\",\n    \"dropoff_zone_sk\",\n    \"rate_code_sk\",\n    \"tpep_pickup_datetime\",\n    \"tpep_dropoff_datetime\",\n    \"passenger_count\",\n    \"trip_distance\",\n    \"payment_type\",\n    \"fare_amount\",\n    \"tip_amount\",\n    \"tolls_amount\",\n    \"total_amount\",\n    \"trip_duration_minutes\",\n    \"pickup_date\",\n    \"curated_ts\",\n    \"record_source\"\n)\n\n# -----------------------------\n# Write Parquet to staging for Redshift COPY\n# -----------------------------\ndf_fact_final.write.mode(\"overwrite\").parquet(S3_FACT_STAGING)\n\n# -----------------------------\n# Print row count\n# -----------------------------\nrow_count = df_fact_final.count()\nprint(f\"âœ… Trip fact table Parquet ready in S3 staging, total rows: {row_count}\")\n"
}