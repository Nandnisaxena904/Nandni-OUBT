{
	"jobConfig": {
		"name": "nyc-taxi-master-to-rds",
		"description": "",
		"role": "arn:aws:iam::474668386387:role/service-role/AWSGlueServiceRole-nandni",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 5,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "nyc-taxi-master-to-rds.py",
		"scriptLocation": "s3://aws-glue-assets-474668386387-us-west-2/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2026-01-15T04:28:45.686Z",
		"developerMode": true,
		"connectionsList": [
			"connwithvpc"
		],
		"temporaryDirectory": "s3://aws-glue-assets-474668386387-us-west-2/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-474668386387-us-west-2/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "# =====================================================\n# Glue Job: Load Master Dimension Tables\n# Source: S3 (Parquet)\n# Target: RDS / Redshift (Glue JDBC Connection)\n# =====================================================\n\nfrom awsglue.context import GlueContext\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.functions import current_timestamp, lit\n\n# -----------------------------------------------------\n# Setup\n# -----------------------------------------------------\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\n\nGLUE_CONNECTION = \"nyc_taxi_rds_conn\"\nBASE_S3_PATH = \"s3://my-data-lake-lab-nandnioubt/master/redshift\"\n\n# -----------------------------------------------------\n# Helper function (Glue JDBC writer)\n# -----------------------------------------------------\ndef write_dim_table(df, table_name):\n    df = df \\\n        .withColumn(\"is_current\", lit(True)) \\\n        .withColumn(\"created_ts\", current_timestamp()) \\\n        .withColumn(\"updated_ts\", current_timestamp())\n\n    dyf = DynamicFrame.fromDF(df, glueContext, table_name)\n\n    glueContext.write_dynamic_frame.from_jdbc_conf(\n        frame=dyf,\n        catalog_connection=GLUE_CONNECTION,\n        connection_options={\n            \"dbtable\": table_name,\n            \"database\": \"mdm_db\"\n        },\n        transformation_ctx=f\"write_{table_name}\"\n    )\n\n# =====================================================\n# 1. Vendor Dimension\n# =====================================================\nvendor_df = spark.read.parquet(f\"{BASE_S3_PATH}/dim_vendor/\")\nwrite_dim_table(vendor_df, \"vendor_dim\")\n\n# =====================================================\n# 2. Zone Dimension\n# =====================================================\nzone_df = spark.read.parquet(f\"{BASE_S3_PATH}/dim_zone/\")\nwrite_dim_table(zone_df, \"zone_dim\")\n\n# =====================================================\n# 3. Rate Code Dimension\n# =====================================================\nratecode_df = spark.read.parquet(f\"{BASE_S3_PATH}/dim_rate_code/\")\nwrite_dim_table(ratecode_df, \"rate_code_dim\")\n\nprint(\"âœ… All dimension tables loaded successfully\")\n"
}