{
	"jobConfig": {
		"name": "nyc-taxi-raw-to-processed",
		"description": "",
		"role": "arn:aws:iam::474668386387:role/service-role/AWSGlueServiceRole-nandni",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "nyc-taxi-raw-to-processed.py",
		"scriptLocation": "s3://aws-glue-assets-474668386387-us-west-2/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2026-01-15T20:02:24.306Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-474668386387-us-west-2/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-474668386387-us-west-2/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "# =====================================================\n# Glue Job: RawToProcessedJob\n# Purpose : Raw -> Processed (clean & validate)\n# =====================================================\n\nfrom awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import functions as F\n\n# -----------------------------------------------------\n# Setup\n# -----------------------------------------------------\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\n\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n\n# -----------------------------------------------------\n# Paths\n# -----------------------------------------------------\nRAW_PATH = \"s3://my-data-lake-lab-nandnioubt/raw/nyc_taxi/\"\nPROCESSED_PATH = \"s3://my-data-lake-lab-nandnioubt/processed/nyc_taxi/\"\n\n# -----------------------------------------------------\n# Read raw data (NO wildcard)\n# -----------------------------------------------------\nraw_df = spark.read.parquet(RAW_PATH)\n\nif raw_df.rdd.isEmpty():\n    raise Exception(\"Raw dataset is empty\")\n\n# -----------------------------------------------------\n# Clean & validate\n# -----------------------------------------------------\nprocessed_df = (\n    raw_df\n    .filter(F.col(\"tpep_pickup_datetime\").isNotNull())\n    .filter(F.col(\"tpep_dropoff_datetime\").isNotNull())\n    .filter(F.col(\"trip_distance\") > 0)\n    .filter(F.col(\"fare_amount\") >= 0)\n    .withColumn(\"pickup_year\", F.year(\"tpep_pickup_datetime\"))\n    .withColumn(\"pickup_month\", F.month(\"tpep_pickup_datetime\"))\n)\n\n# -----------------------------------------------------\n# Write processed data\n# -----------------------------------------------------\n(\n    processed_df\n    .write\n    .mode(\"overwrite\")\n    #.partitionBy(\"pickup_year\", \"pickup_month\")\n    .parquet(PROCESSED_PATH)\n)\n\nprint(\"Raw â†’ Processed load completed successfully\")\n"
}